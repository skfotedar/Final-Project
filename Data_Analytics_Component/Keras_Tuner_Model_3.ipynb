{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izRx3v3wfPRn",
        "outputId": "6658a900-0882-4746-dddc-b5ebdb9f7b87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras-tuner in c:\\users\\frisb\\anaconda3\\envs\\dev\\lib\\site-packages (1.4.7)\n",
            "Requirement already satisfied: keras in c:\\users\\frisb\\anaconda3\\envs\\dev\\lib\\site-packages (from keras-tuner) (3.5.0)\n",
            "Requirement already satisfied: packaging in c:\\users\\frisb\\anaconda3\\envs\\dev\\lib\\site-packages (from keras-tuner) (23.2)\n",
            "Requirement already satisfied: requests in c:\\users\\frisb\\anaconda3\\envs\\dev\\lib\\site-packages (from keras-tuner) (2.32.2)\n",
            "Requirement already satisfied: kt-legacy in c:\\users\\frisb\\anaconda3\\envs\\dev\\lib\\site-packages (from keras-tuner) (1.0.5)\n",
            "Requirement already satisfied: absl-py in c:\\users\\frisb\\anaconda3\\envs\\dev\\lib\\site-packages (from keras->keras-tuner) (2.1.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\frisb\\anaconda3\\envs\\dev\\lib\\site-packages (from keras->keras-tuner) (1.26.4)\n",
            "Requirement already satisfied: rich in c:\\users\\frisb\\anaconda3\\envs\\dev\\lib\\site-packages (from keras->keras-tuner) (13.3.5)\n",
            "Requirement already satisfied: namex in c:\\users\\frisb\\anaconda3\\envs\\dev\\lib\\site-packages (from keras->keras-tuner) (0.0.8)\n",
            "Requirement already satisfied: h5py in c:\\users\\frisb\\anaconda3\\envs\\dev\\lib\\site-packages (from keras->keras-tuner) (3.11.0)\n",
            "Requirement already satisfied: optree in c:\\users\\frisb\\anaconda3\\envs\\dev\\lib\\site-packages (from keras->keras-tuner) (0.12.1)\n",
            "Requirement already satisfied: ml-dtypes in c:\\users\\frisb\\anaconda3\\envs\\dev\\lib\\site-packages (from keras->keras-tuner) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\frisb\\anaconda3\\envs\\dev\\lib\\site-packages (from requests->keras-tuner) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\frisb\\anaconda3\\envs\\dev\\lib\\site-packages (from requests->keras-tuner) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\frisb\\anaconda3\\envs\\dev\\lib\\site-packages (from requests->keras-tuner) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\frisb\\anaconda3\\envs\\dev\\lib\\site-packages (from requests->keras-tuner) (2024.6.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\frisb\\anaconda3\\envs\\dev\\lib\\site-packages (from optree->keras->keras-tuner) (4.11.0)\n",
            "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\frisb\\anaconda3\\envs\\dev\\lib\\site-packages (from rich->keras->keras-tuner) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\frisb\\anaconda3\\envs\\dev\\lib\\site-packages (from rich->keras->keras-tuner) (2.15.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in c:\\users\\frisb\\anaconda3\\envs\\dev\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras->keras-tuner) (0.1.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install keras-tuner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2oKcxUaCep_U"
      },
      "outputs": [],
      "source": [
        "# Import our dependencies\n",
        "import pandas as pd\n",
        "import sklearn as skl\n",
        "import tensorflow as tf\n",
        "\n",
        "# Import the nonlinear dummy data\n",
        "df_glacier = pd.read_csv(\"resources\\dT_Country_parameters_3.csv\")\n",
        "\n",
        "# Split data into X and y\n",
        "X = df_glacier.drop([\"year\",\"glacier_retreat\"], axis=1)\n",
        "y = df_glacier[\"glacier_retreat\"]\n",
        "\n",
        "# Use sklearn to split dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)\n",
        "\n",
        "# Create scaler instance\n",
        "X_scaler = skl.preprocessing.StandardScaler()\n",
        "\n",
        "# Fit the scaler\n",
        "X_scaler.fit(X_train)\n",
        "\n",
        "# Scale the data\n",
        "X_train_scaled = X_scaler.transform(X_train)\n",
        "X_test_scaled = X_scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Zozq8z2Oep_V"
      },
      "outputs": [],
      "source": [
        "# Create a method that creates a new Sequential model with hyperparameter options\n",
        "def create_model(hp):\n",
        "    nn_model = tf.keras.models.Sequential()\n",
        "\n",
        "    # Allow kerastuner to decide which activation function to use in hidden layers\n",
        "    activation = hp.Choice('activation',['relu','tanh'])\n",
        "\n",
        "    # Allow kerastuner to decide number of neurons in first layer\n",
        "    nn_model.add(tf.keras.layers.Dense(units=hp.Int('first_units',\n",
        "        min_value=1,\n",
        "        max_value=30,\n",
        "        step=5), activation=activation, input_dim=len(X.columns)))\n",
        "\n",
        "    # Allow kerastuner to decide number of hidden layers and neurons in hidden layers\n",
        "    for i in range(hp.Int('num_layers', 1, 5)):\n",
        "        nn_model.add(tf.keras.layers.Dense(units=hp.Int('units_' + str(i),\n",
        "            min_value=1,\n",
        "            max_value=30,\n",
        "            step=5),\n",
        "            activation=activation))\n",
        "\n",
        "    nn_model.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
        "\n",
        "    # Compile the model\n",
        "    nn_model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=[\"accuracy\"])\n",
        "\n",
        "    return nn_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lPbVip0xep_V"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reloading Tuner from .\\untitled_project\\tuner0.json\n"
          ]
        }
      ],
      "source": [
        "# Import the kerastuner library\n",
        "import keras_tuner as kt\n",
        "\n",
        "tuner = kt.Hyperband(\n",
        "    create_model,\n",
        "    objective=\"val_accuracy\",\n",
        "    max_epochs=20,\n",
        "    hyperband_iterations=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTzg8T41ep_V",
        "outputId": "59fb6228-0efc-4e40-f619-bcd9d8a2adfd"
      },
      "outputs": [],
      "source": [
        "# Run the kerastuner search for best hyperparameters\n",
        "tuner.search(X_train_scaled,y_train,epochs=20,validation_data=(X_test_scaled,y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USTf9NBMep_W",
        "outputId": "317cd75b-723e-464f-ad91-3a0cb21854dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'activation': 'tanh', 'first_units': 16, 'num_layers': 1, 'units_0': 1, 'units_1': 16, 'units_2': 16, 'units_3': 1, 'units_4': 11, 'tuner/epochs': 20, 'tuner/initial_epoch': 0, 'tuner/bracket': 0, 'tuner/round': 0}\n",
            "{'activation': 'tanh', 'first_units': 26, 'num_layers': 1, 'units_0': 26, 'units_1': 1, 'units_2': 1, 'units_3': 1, 'units_4': 16, 'tuner/epochs': 7, 'tuner/initial_epoch': 0, 'tuner/bracket': 1, 'tuner/round': 0}\n",
            "{'activation': 'tanh', 'first_units': 16, 'num_layers': 5, 'units_0': 1, 'units_1': 1, 'units_2': 21, 'units_3': 26, 'units_4': 16, 'tuner/epochs': 20, 'tuner/initial_epoch': 7, 'tuner/bracket': 1, 'tuner/round': 1, 'tuner/trial_id': '0048'}\n"
          ]
        }
      ],
      "source": [
        "# Get top 3 model hyperparameters and print the values\n",
        "top_hyper = tuner.get_best_hyperparameters(3)\n",
        "for param in top_hyper:\n",
        "    print(param.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbfiY7Eqep_W",
        "outputId": "39bc3788-1059-4d72-c7fc-b6ddd8bfb4d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\frisb\\anaconda3\\envs\\dev\\lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\frisb\\anaconda3\\envs\\dev\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "c:\\Users\\frisb\\anaconda3\\envs\\dev\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:713: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n",
            "c:\\Users\\frisb\\anaconda3\\envs\\dev\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:713: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 30 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 - 0s - 213ms/step - accuracy: 0.9091 - loss: 0.5031\n",
            "Loss: 0.5031033158302307, Accuracy: 0.9090909361839294\n",
            "1/1 - 0s - 151ms/step - accuracy: 0.8182 - loss: 0.6467\n",
            "Loss: 0.6466594338417053, Accuracy: 0.8181818127632141\n",
            "1/1 - 0s - 203ms/step - accuracy: 0.8182 - loss: 0.6571\n",
            "Loss: 0.657103955745697, Accuracy: 0.8181818127632141\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the top 3 models against the test dataset\n",
        "top_model = tuner.get_best_models(3)\n",
        "for model in top_model:\n",
        "    model_loss, model_accuracy = model.evaluate(X_test_scaled,y_test,verbose=2)\n",
        "    print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GpGaRNBJep_W"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
